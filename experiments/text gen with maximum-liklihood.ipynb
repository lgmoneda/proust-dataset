{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on @yaov notebook \"The unreasonable effectiveness of Character-level Language Models\" (http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139).\n",
    "\n",
    "Using the ** unsmoothed maximum-liklihood character level language models ** to generate text passing first words, which can be seen as defining the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import *\n",
    "import random\n",
    "\n",
    "def train_char_lm(fname, order=4):\n",
    "    data = file(fname).read()\n",
    "    lm = defaultdict(Counter)\n",
    "    pad = \"~\" * order\n",
    "    data = pad + data\n",
    "    for i in xrange(len(data)-order):\n",
    "        history, char = data[i:i+order], data[i+order]\n",
    "        lm[history][char]+=1\n",
    "    def normalize(counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        return [(c,cnt/s) for c,cnt in counter.iteritems()]\n",
    "    outlm = {hist:normalize(chars) for hist, chars in lm.iteritems()}\n",
    "    return outlm\n",
    "\n",
    "def generate_letter(lm, history, order):\n",
    "        history = history[-order:]   \n",
    "\n",
    "        if len(history) < order:\n",
    "            possible_hist = [key for key, value in lm.items() if history.lower() == key.lower()[-len(history):]]\n",
    "            history = possible_hist[random.randint(0, len(possible_hist)-1)]\n",
    "      \n",
    "        try:\n",
    "            dist = lm[history]\n",
    "        except:\n",
    "            dist = lm[history.lower()]\n",
    "        x = random.random()\n",
    "        for c,v in dist:\n",
    "            x = x - v\n",
    "            if x <= 0: return c\n",
    "\n",
    "def generate_text(lm, order, nletters=1000, history=None):\n",
    "    if history == None: \n",
    "        history = \"~\" * order\n",
    "    else:\n",
    "        history = history[:min(len(history), order)]\n",
    "    out = []\n",
    "    out.append(history)\n",
    "    while(len(out) < nletters or out[-1] != \".\"):\n",
    "    #for i in xrange(nletters):\n",
    "        c = generate_letter(lm, history, order)\n",
    "        history = history[-order:] + c\n",
    "        out.append(c)\n",
    "    return \"\".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm = train_char_lm(\"data/proust_dataset_ptbr.txt\", order=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print generate_text(lm, 20, nletters=300, history=\"Seios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
